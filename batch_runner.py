"""SiliconFlow Batch å·¥ä½œæµï¼šç”Ÿæˆ JSONLã€æäº¤ä»»åŠ¡å¹¶åˆå¹¶ç»“æœã€‚"""

from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from .config import AppConfig
from .prompt import build_system_prompt
from .schema import ExtractionResult, flatten_for_csv, normalise_extraction
from .silicon_client import SiliconClient
from .sync_runner import Record, read_csv_records, write_outputs


def build_batch_payload(record: Record, config: AppConfig, system_prompt: str, *, custom_id: str | None = None) -> Dict[str, object]:
	return {
		"custom_id": custom_id or record.id_value,
		"method": "POST",
		"url": "/v1/chat/completions",
		"body": {
			"model": config.model,
			"messages": [
				{"role": "system", "content": system_prompt},
				{"role": "user", "content": record.text},
			],
			"temperature": config.response_temperature,
			"response_format": {"type": "json_object"},
		},
	}


def write_request_jsonl(
	records: List[Record],
	output_dir: Path,
	config: AppConfig,
	filename: str = "batch_requests.jsonl",
) -> Tuple[Path, Dict[str, Record]]:
	output_dir.mkdir(parents=True, exist_ok=True)
	path = output_dir / filename
	system_prompt = build_system_prompt(config.template_id)
	# Batch è¦æ±‚ custom_id å…¨å±€å”¯ä¸€ï¼›ä¸ºé‡å¤ ID å¢åŠ åºå·åç¼€
	seen_counts: dict[str, int] = {}
	id_map: Dict[str, Record] = {}
	with path.open("w", encoding="utf-8") as fh:
		for record in records:
			base_id = record.id_value or "row"
			count = seen_counts.get(base_id, 0)
			seen_counts[base_id] = count + 1
			unique_id = base_id if count == 0 else f"{base_id}__{count+1}"
			id_map[unique_id] = record
			payload = build_batch_payload(record, config, system_prompt, custom_id=unique_id)
			fh.write(json.dumps(payload, ensure_ascii=False) + "\n")
	return path, id_map


def poll_batch(client: SiliconClient, batch_id: str, interval: float = 10.0) -> Dict[str, object]:
	"""è½®è¯¢æ‰¹é‡ä»»åŠ¡çŠ¶æ€ï¼Œç›´åˆ°å®Œæˆæˆ–å¤±è´¥ã€‚"""
	poll_count = 0
	while True:
		status = client.retrieve_batch(batch_id)
		state = status.get("status")
		poll_count += 1
		
		if state == "completed":
			print(f"âœ… æ‰¹é‡ä»»åŠ¡å®Œæˆï¼")
			return status
		elif state in {"failed", "cancelled"}:
			print(f"âŒ æ‰¹é‡ä»»åŠ¡çŠ¶æ€ï¼š{state}")
			return status
		else:
			# æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯ 3 æ¬¡è½®è¯¢æ˜¾ç¤ºä¸€æ¬¡ï¼Œé¿å…åˆ·å±ï¼‰
			if poll_count % 3 == 1:
				print(f"â³ æ‰¹é‡ä»»åŠ¡å¤„ç†ä¸­... (çŠ¶æ€: {state}, å·²è½®è¯¢: {poll_count} æ¬¡)")
		time.sleep(interval)



def parse_batch_results(
	records: List[Record],
	id_map: Dict[str, Record],
	result_bytes: bytes,
	template_id: str = "rcc",
) -> List[Tuple[Record, Optional[ExtractionResult], Optional[str]]]:
	results_dict: Dict[str, Tuple[Record, Optional[ExtractionResult], Optional[str]]] = {}
	lines = result_bytes.decode("utf-8").splitlines()
	for line in lines:
		if not line.strip():
			continue
		entry = json.loads(line)
		custom_id = entry.get("custom_id")
		record = id_map.get(custom_id)
		if not record:
			continue
		response = entry.get("response")
		error = entry.get("error")
		if response and not error:
			try:
				output = response["body"]["choices"][0]["message"]["content"]
				data = json.loads(output)
				data.setdefault("id_value", record.id_value)
				extraction = normalise_extraction(data, template_id)
				extraction.id_value = record.id_value
				results_dict[record.id_value] = (record, extraction, None)
			except Exception as exc:  # pylint: disable=broad-except
				results_dict[record.id_value] = (record, None, str(exc))
		else:
			error_msg = str(error) if error else "æœªçŸ¥é”™è¯¯"
			results_dict[record.id_value] = (record, None, error_msg)

	ordered: List[Tuple[Record, Optional[ExtractionResult], Optional[str]]] = []
	for record in records:
		ordered.append(
			results_dict.get(record.id_value, (record, None, "æ‰¹é‡ä»»åŠ¡æ— è¿”å›"))
		)
	return ordered


def run_batch(
	*,
	input_path: Path,
	config: Optional[AppConfig] = None,
	text_col: Optional[str] = None,
	text_cols: Optional[str] = None,  # æ–°å¢ï¼šå¤šåˆ—æ¨¡å¼
	id_col: Optional[str] = None,
	limit: Optional[int] = None,
	output_dir: Optional[Path] = None,
	request_dir: Optional[Path] = None,
	poll_interval: float = 15.0,
) -> Dict[str, object]:
	app_config = config or AppConfig.from_env()
	provider = app_config.get_provider()
	
	# æ£€æŸ¥æ‰¹é‡æ¨ç†æ”¯æŒ
	if provider not in {"siliconflow", "aliyun", "openai"}:
		raise RuntimeError(
			f"æ‰¹é‡æ¨ç†æ¨¡å¼å½“å‰ä»…æ”¯æŒ siliconflowã€aliyunã€openaiã€‚"
			f"å½“å‰ä¾›åº”å•†ï¼š{provider or 'custom'}ã€‚"
			f"è¯·ä½¿ç”¨ --mode sync è¿›è¡ŒåŒæ­¥æ¨¡å¼å¤„ç†ã€‚"
		)
	
	records = read_csv_records(
		input_path,
		app_config,
		text_col=text_col,
		text_cols=text_cols,
		id_col=id_col,
		limit=limit,
	)
	
	if not records:
		raise RuntimeError("æœªè¯»å–åˆ°ä»»ä½•è®°å½•ï¼Œè¯·æ£€æŸ¥ CSV æ–‡ä»¶å’Œåˆ—åé…ç½®")
	
	print(f"ğŸ“¤ å‡†å¤‡æ‰¹é‡å¤„ç† {len(records)} æ¡è®°å½•ï¼Œä¾›åº”å•†ï¼š{provider or 'custom'}")
	
	client = SiliconClient(app_config)
	request_dir = request_dir or Path("inputs")
	request_path, id_map = write_request_jsonl(records, request_dir, app_config)
	
	print(f"ğŸ“ å·²ç”Ÿæˆæ‰¹é‡è¯·æ±‚æ–‡ä»¶ï¼š{request_path}")
	print(f"â¬†ï¸  æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ° {provider or 'API'}...")
	
	file_info = client.upload_jsonl(path=str(request_path))
	
	# å…¼å®¹ä¸åŒè¿”å›ç»“æ„ï¼Œç¡®ä¿æ‹¿åˆ°æ–‡ä»¶ID
	file_id = (
		(file_info.get("id") if isinstance(file_info, dict) else None)
		or (file_info.get("data", {}).get("id") if isinstance(file_info, dict) else None)
	)
	if not file_id:
		raise RuntimeError(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸä½†æœªè·å–åˆ°æ–‡ä»¶IDï¼Œè¿”å›ï¼š{file_info}")
	
	print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒIDï¼š{file_id}")
	print(f"ğŸš€ æ­£åœ¨åˆ›å»ºæ‰¹é‡ä»»åŠ¡...")
	
	# æ ¹æ®ä¾›åº”å•†è°ƒæ•´æ‰¹é‡ä»»åŠ¡åˆ›å»ºå‚æ•°
	if provider == "aliyun":
		# é˜¿é‡Œäº‘ç™¾ç‚¼ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹å‚æ•°ï¼Œä¸éœ€è¦ extra_body
		batch = client.create_batch(
			input_file_id=file_id,
			extra_body=None,  # é˜¿é‡Œäº‘åœ¨ body ä¸­å·²ç»æŒ‡å®šäº† model
		)
	else:
		# SiliconFlow/OpenAIï¼šä½¿ç”¨ extra_body è¦†ç›–æ¨¡å‹
		batch = client.create_batch(
			input_file_id=file_id,
			extra_body={"replace": {"model": app_config.model}},
		)
	batch_id = batch["id"]
	print(f"ğŸ“‹ æ‰¹é‡ä»»åŠ¡ IDï¼š{batch_id}")
	print(f"â±ï¸  å¼€å§‹è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆé—´éš”ï¼š{poll_interval} ç§’ï¼‰...")
	
	status = poll_batch(client, batch_id, interval=poll_interval)
	
	if status.get("status") != "completed":
		return {"batch": batch, "status": status, "results": None}
	
	print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ç»“æœæ–‡ä»¶...")
	output_file_id = status.get("output_file_id")
	if not output_file_id:
		for attempt in range(6):
			time.sleep(2.0 * (attempt + 1))
			refreshed = client.retrieve_batch(batch_id)
			output_file_id = refreshed.get("output_file_id")
			if output_file_id:
				status = refreshed
				break
	if not output_file_id:
		raise RuntimeError("æ‰¹é‡ä»»åŠ¡å·²å®Œæˆä½†å°šæœªç”Ÿæˆç»“æœæ–‡ä»¶ï¼Œè¯·ç¨åé‡è¯•")
	
	# ä¸‹è½½ç»“æœï¼šå³ä¾¿ completedï¼Œä¹Ÿå¯èƒ½çŸ­æš‚ 404ï¼Œå¢åŠ é‡è¯•
	for attempt in range(8):
		try:
			result_bytes = client.download_file(output_file_id)
			break
		except Exception:
			time.sleep(2.0 * (attempt + 1))
	else:
		raise RuntimeError("ç»“æœæ–‡ä»¶æš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ä¸‹è½½")
	
	print(f"âœ… ç»“æœæ–‡ä»¶ä¸‹è½½æˆåŠŸï¼Œæ­£åœ¨è§£æ...")
	parsed = parse_batch_results(records, id_map, result_bytes, template_id=app_config.template_id)
	
	output_path = output_dir or Path(app_config.output_dir)
	write_outputs(parsed, output_dir=output_path, config=app_config)
	
	succeeded = sum(1 for _, extraction, err in parsed if extraction and not err)
	print(f"âœ¨ æ‰¹é‡å¤„ç†å®Œæˆï¼šæˆåŠŸ {succeeded}/{len(parsed)} æ¡ï¼Œç»“æœä¿å­˜è‡³ {output_path}")
	
	return {"batch": batch, "status": status, "results": parsed}


